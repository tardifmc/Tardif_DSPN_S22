{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2W919d2ZXp7"
   },
   "source": [
    "# Homework 7: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4nOzVhyZXqK"
   },
   "source": [
    "This homework assignment is designed to give you practice with classification models. We'll try to predict which words are more likely to be responded to correctly during a lexical decision task, based on their length and frequency.\n",
    "\n",
    "We will be using data from the English Lexicon Project again. However, this time we will use response correctness as our dependent variable. Load **LexicalData_withIncorrect.csv**, which includes incorrect trials as well as correct ones, and also **Items.csv**. Both can be found in the *Homework/lexDat* folder in the class GitHub repository. \n",
    "\n",
    "This data is a subset of the [English Lexicon Project database](https://elexicon.wustl.edu/). It provides response correctness and reaction times (in milliseconds) of many subjects as they are presented with letter strings and asked to decide, as quickly and as accurately as possible, whether the letter string is a word or not. The **Items.csv** provides characteristics of the words used, namely frequency (how common is this word?) and length (how many letters?). \n",
    "\n",
    "*Data courtesy of Balota, D.A., Yap, M.J., Cortese, M.J., Hutchison, K.A., Kessler, B., Loftis, B., Neely, J.H., Nelson, D.L., Simpson, G.B., & Treiman, R. (2007). The English Lexicon Project. Behavior Research Methods, 39, 445-459.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DsyBTB6ZXqN"
   },
   "source": [
    "---\n",
    "## 1. Loading and formatting the data (1 point)\n",
    "\n",
    "Load in data from the **LexicalData_withIncorrect.csv** and **Items.csv** files. Use `left_join` to add word characteristics `Length` and `Log_Freq_Hal` from **Items** to the **LexicalData**, and use `drop_na()` to get rid of any observations with missing values. Then use `head()` to look at the first few rows of the data. \n",
    "\n",
    "*Note: We're just working with `Correct` in this homework, so no need to worry about reformatting reaction times.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "executionInfo": {
     "elapsed": 7413,
     "status": "ok",
     "timestamp": 1617034976794,
     "user": {
      "displayName": "Patience Stevens",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi-_9ZqhIFhAv1oMehJNvNuIKSTyrFQHzjxQKhx=s64",
      "userId": "01994571539255174942"
     },
     "user_tz": 240
    },
    "id": "UnBVazYfZXqP",
    "outputId": "0fedfa64-26db-49cc-da34-02cf74bf29d4"
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in library(ggplot): there is no package called 'ggplot'\n",
     "output_type": "error",
     "traceback": [
      "Error in library(ggplot): there is no package called 'ggplot'\nTraceback:\n",
      "1. library(ggplot)"
     ]
    }
   ],
   "source": [
    "library(tidyverse)\n",
    "\n",
    "setwd(\"C:/Users/MCT40/OneDrive - University of Pittsburgh/Classes/Spring2022/CMU_STATS/HOMEWORK\")\n",
    "LD <- read.csv(\"lexDat/LexicalData_withIncorrect.csv\")\n",
    "items <- read.csv(\"lexDat/items.csv\")\n",
    "\n",
    "names(LD)[names(LD) == \"D_Word\"] <- \"Word\" \n",
    "\n",
    "LD <- left_join(LD, items, by='Word') #adding together all of the variables to make the final question of the homework easier\n",
    "\n",
    "LD %>% \n",
    "  left_join(\n",
    "    select(items, Word, Length, Log_Freq_HAL),\n",
    "    by = \"Word\"\n",
    "  ) -> LD2\n",
    "\n",
    "LD2 %>% drop_na() -> LD3 #remove NAs save to LD2\n",
    "any(LD3 == \"n/a\")\n",
    "\n",
    "head(LD3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeK0H64WlOBM"
   },
   "source": [
    "---\n",
    "## 2. Visualizing the data (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dP_6o9rpmThw"
   },
   "source": [
    "First, we'll try to visualize whether trials that are responded to correctly versus incorrectly differ from each other in terms of word length and log frequency. The code is included below, so that this homework doesn't get too cumbersome. All you have to do is **change the name of the data set**, **run the code**, and **write some observations about the output**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "executionInfo": {
     "elapsed": 4369,
     "status": "ok",
     "timestamp": 1616456271097,
     "user": {
      "displayName": "Patience Stevens",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi-_9ZqhIFhAv1oMehJNvNuIKSTyrFQHzjxQKhx=s64",
      "userId": "01994571539255174942"
     },
     "user_tz": 240
    },
    "id": "PIPheP5ipgKg",
    "outputId": "e6d7b9a1-00fd-4358-ac36-8d802a725541"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR while rich displaying an object: Error in FUN(X[[i]], ...): object 'Log_Freq_HAL' not found\n",
      "\n",
      "Traceback:\n",
      "1. tryCatch(withCallingHandlers({\n",
      " .     if (!mime %in% names(repr::mime2repr)) \n",
      " .         stop(\"No repr_* for mimetype \", mime, \" in repr::mime2repr\")\n",
      " .     rpr <- repr::mime2repr[[mime]](obj)\n",
      " .     if (is.null(rpr)) \n",
      " .         return(NULL)\n",
      " .     prepare_content(is.raw(rpr), rpr)\n",
      " . }, error = error_handler), error = outer_handler)\n",
      "2. tryCatchList(expr, classes, parentenv, handlers)\n",
      "3. tryCatchOne(expr, names, parentenv, handlers[[1L]])\n",
      "4. doTryCatch(return(expr), name, parentenv, handler)\n",
      "5. withCallingHandlers({\n",
      " .     if (!mime %in% names(repr::mime2repr)) \n",
      " .         stop(\"No repr_* for mimetype \", mime, \" in repr::mime2repr\")\n",
      " .     rpr <- repr::mime2repr[[mime]](obj)\n",
      " .     if (is.null(rpr)) \n",
      " .         return(NULL)\n",
      " .     prepare_content(is.raw(rpr), rpr)\n",
      " . }, error = error_handler)\n",
      "6. repr::mime2repr[[mime]](obj)\n",
      "7. repr_text.default(obj)\n",
      "8. paste(capture.output(print(obj)), collapse = \"\\n\")\n",
      "9. capture.output(print(obj))\n",
      "10. evalVis(expr)\n",
      "11. withVisible(eval(expr, pf))\n",
      "12. eval(expr, pf)\n",
      "13. eval(expr, pf)\n",
      "14. print(obj)\n",
      "15. print.ggplot(obj)\n",
      "16. ggplot_build(x)\n",
      "17. ggplot_build.ggplot(x)\n",
      "18. by_layer(function(l, d) l$compute_aesthetics(d, plot))\n",
      "19. f(l = layers[[i]], d = data[[i]])\n",
      "20. l$compute_aesthetics(d, plot)\n",
      "21. f(..., self = self)\n",
      "22. scales_add_defaults(plot$scales, data, aesthetics, plot$plot_env)\n",
      "23. lapply(aesthetics[new_aesthetics], eval_tidy, data = data)\n",
      "24. FUN(X[[i]], ...)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAA1BMVEX///+nxBvIAAAACXBI\nWXMAABJ0AAASdAHeZh94AAACw0lEQVR4nO3BgQAAAADDoPlTH+ECVQEAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMA3yB4AAXYzOhIAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "LD3$Correct <- as.factor(LD3$Correct) # so that R knows that Correct is categorical, not numeric. \n",
    "\n",
    "# plot the Correct / Incorrect clusters\n",
    "ggplot(LD3,aes(x=round(Log_Freq_HAL,1),y=Length,col=Correct)) + \n",
    "geom_point(position=\"jitter\",alpha=0.5) + theme_light() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8W0y8eRxTa6Z"
   },
   "source": [
    "What do you observe about the \"Correct\" and \"Incorrect\" clusters? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWxi8O6voooe"
   },
   "source": [
    "> Shorter words and those with lower Log_Freq_Hal values tend to be more often incorrect than those with higher values in those variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3b3_KsHk-xD"
   },
   "source": [
    "---\n",
    "## 3. Logistic Regression: Fitting the model (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlahTNHxrJaR"
   },
   "source": [
    "Fit a logistic regression model to the data using `Length`, `Log_Freq_HAL`, and their interaction to predict `Correct`. Use `glm()` to fit the model, and look at its output using `summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "executionInfo": {
     "elapsed": 759,
     "status": "ok",
     "timestamp": 1617034981292,
     "user": {
      "displayName": "Patience Stevens",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi-_9ZqhIFhAv1oMehJNvNuIKSTyrFQHzjxQKhx=s64",
      "userId": "01994571539255174942"
     },
     "user_tz": 240
    },
    "id": "AidH_KidrX9L",
    "outputId": "ed096fe4-8a0e-47b6-f3ff-d742010edd7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = Correct ~ Length * Log_Freq_HAL, family = binomial, \n",
       "    data = LD3)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-3.0494   0.3178   0.4182   0.5208   1.3225  \n",
       "\n",
       "Coefficients:\n",
       "                     Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept)         -0.939996   0.115623  -8.130 4.30e-16 ***\n",
       "Length               0.151266   0.012789  11.828  < 2e-16 ***\n",
       "Log_Freq_HAL         0.394772   0.018762  21.041  < 2e-16 ***\n",
       "Length:Log_Freq_HAL -0.010558   0.002202  -4.795 1.63e-06 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 49809  on 70588  degrees of freedom\n",
       "Residual deviance: 46563  on 70585  degrees of freedom\n",
       "  (4280 observations deleted due to missingness)\n",
       "AIC: 46571\n",
       "\n",
       "Number of Fisher Scoring iterations: 5\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "Log_reg <- glm(Correct ~ Length*Log_Freq_HAL, data= LD3, family = binomial)\n",
    "summary(Log_reg)\n",
    "\n",
    "#Because GITHUB womp womp: \n",
    "\n",
    "# Call:\n",
    "#   glm(formula = Correct ~ Length * Log_Freq_HAL, family = binomial, \n",
    "#       data = LD3)\n",
    "# \n",
    "# Deviance Residuals: \n",
    "#   Min       1Q   Median       3Q      Max  \n",
    "# -3.0494   0.3178   0.4182   0.5208   1.3225  \n",
    "# \n",
    "# Coefficients:\n",
    "#   Estimate Std. Error z value Pr(>|z|)    \n",
    "# (Intercept)         -0.939996   0.115623  -8.130 4.30e-16 ***\n",
    "#   Length               0.151266   0.012789  11.828  < 2e-16 ***\n",
    "#   Log_Freq_HAL         0.394772   0.018762  21.041  < 2e-16 ***\n",
    "#   Length:Log_Freq_HAL -0.010558   0.002202  -4.795 1.63e-06 ***\n",
    "#   ---\n",
    "#   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
    "# \n",
    "# (Dispersion parameter for binomial family taken to be 1)\n",
    "# \n",
    "# Null deviance: 49809  on 70588  degrees of freedom\n",
    "# Residual deviance: 46563  on 70585  degrees of freedom\n",
    "# (4280 observations deleted due to missingness)\n",
    "# AIC: 46571\n",
    "# \n",
    "# Number of Fisher Scoring iterations: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AC8nTnYtzAb"
   },
   "source": [
    "What can you conclude from this output? (a brief gist is fine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MLyXXTIuACv"
   },
   "source": [
    "> The intercept of the mode being significant represents the mean oh the correct response (which isn't sensical) when the log odds and length of the word are 0, which isn't applicable or interesting.\n",
    "\n",
    "> The main effect of length indicates that as length increases so to does the log odds of number of correct responses determining whether or not it was a real word.\n",
    "\n",
    ">The main effect of Log_Freq_HAL  indicates that as that variable increases so too do the log odds of number of correct responses determining whether or not it was a real word.\n",
    "\n",
    "> The interaction between the two terms indicates that as one of the variables increases the influence of the other variable decreases. Such that one predictor variable has a different influences on the outcome variable depending on the behaviour of the second predictor variable (Length and Log_Freq_HAL).\n",
    "\n",
    "> because the contrasts were not coded we cannot say much more about these effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCnLulvxA2fM"
   },
   "source": [
    "---\n",
    "## 4. Interpreting predictions from the model (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCy9W_Mou4zT"
   },
   "source": [
    "Finally, look at how well this logistic regression model does at predicting correctness. Use `predict()` and a threshold of 0.5 to generate predicted `Correct` values for each trial, then output a confusion matrix and overall accuracy for these predictions.\n",
    "\n",
    "*Hint: see the Classifiers tutorial.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "executionInfo": {
     "elapsed": 402,
     "status": "ok",
     "timestamp": 1617034997434,
     "user": {
      "displayName": "Patience Stevens",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi-_9ZqhIFhAv1oMehJNvNuIKSTyrFQHzjxQKhx=s64",
      "userId": "01994571539255174942"
     },
     "user_tz": 240
    },
    "id": "nZQ2_WzixTJH",
    "outputId": "f81245ff-330c-48a7-a9a8-410622ea5ef6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "14"
      ],
      "text/latex": [
       "14"
      ],
      "text/markdown": [
       "14"
      ],
      "text/plain": [
       "[1] 14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 1</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>predicted_prob</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>0.9479784</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>0.8826628</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>0.9417677</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>0.7707288</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>0.6682356</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>0.8704362</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 1\n",
       "\\begin{tabular}{r|l}\n",
       "  & predicted\\_prob\\\\\n",
       "  & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 0.9479784\\\\\n",
       "\t2 & 0.8826628\\\\\n",
       "\t3 & 0.9417677\\\\\n",
       "\t4 & 0.7707288\\\\\n",
       "\t5 & 0.6682356\\\\\n",
       "\t6 & 0.8704362\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 1\n",
       "\n",
       "| <!--/--> | predicted_prob &lt;dbl&gt; |\n",
       "|---|---|\n",
       "| 1 | 0.9479784 |\n",
       "| 2 | 0.8826628 |\n",
       "| 3 | 0.9417677 |\n",
       "| 4 | 0.7707288 |\n",
       "| 5 | 0.6682356 |\n",
       "| 6 | 0.8704362 |\n",
       "\n"
      ],
      "text/plain": [
       "  predicted_prob\n",
       "1 0.9479784     \n",
       "2 0.8826628     \n",
       "3 0.9417677     \n",
       "4 0.7707288     \n",
       "5 0.6682356     \n",
       "6 0.8704362     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ERROR",
     "evalue": "Error in `$<-.data.frame`(`*tmp*`, predicted_binary, value = c(0, 0, 0, : replacement has 14 rows, data has 70589\n",
     "output_type": "error",
     "traceback": [
      "Error in `$<-.data.frame`(`*tmp*`, predicted_binary, value = c(0, 0, 0, : replacement has 14 rows, data has 70589\nTraceback:\n",
      "1. `$<-`(`*tmp*`, predicted_binary, value = c(0, 0, 0, 0, 0, 0, \n . 0, 0, 0, 0, 0, 0, 0, 0))",
      "2. `$<-.data.frame`(`*tmp*`, predicted_binary, value = c(0, 0, 0, \n . 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0))",
      "3. stop(sprintf(ngettext(N, \"replacement has %d row, data has %d\", \n .     \"replacement has %d rows, data has %d\"), N, nrows), domain = NA)"
     ]
    }
   ],
   "source": [
    "## Assign threshold to equal 0.50\n",
    "threshold = 0.50\n",
    "\n",
    "## Create new data frame containing predicted values for each observation, \n",
    "## based on the fit of the logistic regression model run in part 3. \n",
    "LD_predict = data.frame(predict(Log_reg, type = \"response\"))\n",
    "\n",
    "## Rename the predicted values generated above to \"predicted_prob\" for ease of interpretability\n",
    "colnames(LD_predict) = c('predicted_prob')\n",
    "head(LD_predict)\n",
    "\n",
    "# Make a list of \"Incorrect\" responses (coded as 0)\n",
    "num_obs = length(LD3)\n",
    "LD_predict$predicted_binary=rep(0,num_obs)\n",
    "LD_predict$predicted_binary[LD_predict$predicted_prob>threshold]= 1\n",
    "\n",
    "# Look at the prediction accuracy in form of confusion matrix table\n",
    "LD_conf = data.frame(LD_predict$predicted_binary, LD3$Correct)\n",
    "colnames(LD_conf) = c('predicted', 'actual')\n",
    "table(LD_conf)\n",
    "\n",
    "## Abbreviated summary: \n",
    "## 13 predicted 0 actual 0 (true neg)\n",
    "## 44 predicted 0 actual 1 (false neg)\n",
    "## 7966 predicted 1 actual 0 (fale pos)\n",
    "## 62566 predicted 1 actual 1 (true pos)\n",
    "\n",
    "## Calculate the general accuracy of the model's predictions: \n",
    "print(paste(\"Accuracy:\",mean(LD_conf$predicted == LD_conf$actual)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-D1Kk4Wz6eA"
   },
   "source": [
    "Did the model do well at predicting lexical decision correctness? Why or why not? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHZnDFzV0K7K"
   },
   "source": [
    "> *Write your response here*  \n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_xE_hjFDMe-"
   },
   "source": [
    "## 5. QDA (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzvQBhcbD995"
   },
   "source": [
    "Load in the `MASS` library and fit a QDA model to the data set. The predictors are still `Length`, `Log_Freq_HAL`, and their interaction, just like the logistic regression model you just ran, and the dependent variable is still `Correct`. \n",
    "\n",
    "*Hint: see the Classifiers tutorial.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v5unJLgdERP_"
   },
   "outputs": [],
   "source": [
    "## Load in MASS library\n",
    "library(MASS)\n",
    "\n",
    "## Fit QDA model on data set (no test train split here)\n",
    "lexDat_qda.fit = qda(Correct ~ Length*Log_Freq_HAL, data=lexDat_accuracy_final)\n",
    "lexDat_qda.fit\n",
    "\n",
    "\n",
    "## Create a data frame containing posterior probabilities of 0 and 1 for each observation\n",
    "qda_prob_df = data.frame(predict(lexDat_qda.fit, type = \"response\"))\n",
    "head(qda_prob_df)\n",
    "\n",
    "## Take that new data frame and create a new column such that if the\n",
    "## posterior probability of 1 is greater than 0.5, it will assign a 1 and otherwise\n",
    "## will assign a 0:\n",
    "\n",
    "qda_prob_df %>%\n",
    "  mutate(predicted_binary = if_else(posterior.1 >= threshold, 1, 0)) -> qda_prob_df2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YceNR0uSEquX"
   },
   "source": [
    "Now look at how well the predicted `Correct` values compare with actual `Correct` values for the whole data set. Output a confusion matrix and overall prediction accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "executionInfo": {
     "elapsed": 460,
     "status": "ok",
     "timestamp": 1616455028335,
     "user": {
      "displayName": "Patience Stevens",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi-_9ZqhIFhAv1oMehJNvNuIKSTyrFQHzjxQKhx=s64",
      "userId": "01994571539255174942"
     },
     "user_tz": 240
    },
    "id": "tvhhrDWRF2hJ",
    "outputId": "7ed64e88-6320-40b2-e900-a5d07d994635"
   },
   "outputs": [],
   "source": [
    "## Generate confusion matrix\n",
    "confusion_df2 = data.frame(qda_prob_df2$predicted_binary, lexDat_accuracy_final$Correct)\n",
    "colnames(confusion_df2) = c('predicted', 'actual')\n",
    "table(confusion_df2)\n",
    "\n",
    "\n",
    "print(paste(\"Accuracy:\",mean(confusion_df2$predicted == confusion_df2$actual)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leurrTYKHBe_"
   },
   "source": [
    "How does QDA prediction performance differ from that of logistic regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3BRrwUhHV1J"
   },
   "source": [
    "> *Write your response here* \n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4MPECMmZXqe"
   },
   "source": [
    "**DUE:** 5pm EST, March 25, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9GUofXN4BVy"
   },
   "source": [
    "**IMPORTANT** Did you collaborate with anyone on this assignment? If so, list their names here. \n",
    "> Emily Goldberg, and her fascinating taste in marshmallow, matcha, cinnamon candle scents"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Homework7_solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
